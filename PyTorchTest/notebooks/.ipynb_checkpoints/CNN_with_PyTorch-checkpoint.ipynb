{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loading FashionMNIST data in <i>data</i> folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=10\n",
    "    ,shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set count:\n",
      "60000\n",
      "\n",
      "Train set bin count:\n",
      "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])\n",
      "\n",
      "Sample image shape:\n",
      "torch.Size([1, 28, 28])\n",
      "\n",
      "Sample label:\n",
      "9\n",
      "\n",
      "Sample image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12c8b4f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nTrain set count:\")\n",
    "print(len(train_set))\n",
    "print(\"\\nTrain set bin count:\")\n",
    "print(train_set.targets.bincount())\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "image, label = sample\n",
    "\n",
    "print(\"\\nSample image shape:\")\n",
    "print(image.shape)\n",
    "print(\"\\nSample label:\")\n",
    "print(label)\n",
    "print(\"\\nSample image:\")\n",
    "plt.imshow(image.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)  # out_channels - is the number of filters\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the gradiant calculation\n",
    "\n",
    "PyTorch uses a dynamic computational graph. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the networkâ€™s weights. We're turning it off for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x12c90a5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net1 = Network()\n",
    "# net2 = Network()  # To verify the waights generated are different from net1\n",
    "print(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the sample \n",
    "To put our single sample image tensor into a batch with a size of 1, we just need to unsqueeze() the tensor to add an additional dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts an additional dimension that represents a batch of size 1\n",
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call forward of the network\n",
    "\n",
    "<b>Don't call forward method directly.</b> Instead of calling the forward() method directly, we call the object instance. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. This applies to all PyTorch neural network modules, namely, networks and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net1(image.unsqueeze(0))\n",
    "# pred = net2(image.unsqueeze(0))   # The weights and the predictions will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction output:\n",
      "tensor([[ 0.1406,  0.1332,  0.0946, -0.0950,  0.1043,  0.0888, -0.1078, -0.0841, -0.1085,  0.0542]])\n",
      "\n",
      "Prediction output shape:\n",
      "torch.Size([1, 10])\n",
      "\n",
      "Original sample label:\n",
      "9\n",
      "\n",
      "Predicted output label:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrediction output:\")\n",
    "print(pred)\n",
    "\n",
    "print(\"\\nPrediction output shape:\")\n",
    "print(pred.shape)\n",
    "\n",
    "print(\"\\nOriginal sample label:\")\n",
    "print(label)\n",
    "\n",
    "print(\"\\nPredicted output label:\")\n",
    "print(pred.argmax(dim=1).item())  # item() function convertes a scalar tensor to a scalar value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax for probability\n",
    "For each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could use the softmax() function from the nn.functional package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1120, 0.1112, 0.1070, 0.0885, 0.1080, 0.1064, 0.0874, 0.0895, 0.0873, 0.1027]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sum of all ten probabilities is 1\n",
    "F.softmax(pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take 10 image-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Images in a batch: 10 and labels: 10\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "print(f\"\\nImages in a batch: {len(images)} and labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4, 2, 8, 2, 3, 9, 4, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f664610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Network()\n",
    "preds = net2(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0366,  0.1488,  0.0967, -0.0408, -0.1805, -0.0914,  0.0687, -0.0618, -0.0973, -0.0775],\n",
       "        [-0.0424,  0.1489,  0.0990, -0.0348, -0.1835, -0.0877,  0.0637, -0.0624, -0.0992, -0.0743],\n",
       "        [-0.0442,  0.1498,  0.1001, -0.0318, -0.1861, -0.0893,  0.0641, -0.0623, -0.1005, -0.0713],\n",
       "        [-0.0376,  0.1483,  0.0984, -0.0376, -0.1813, -0.0908,  0.0670, -0.0615, -0.1009, -0.0761],\n",
       "        [-0.0436,  0.1499,  0.0992, -0.0339, -0.1853, -0.0903,  0.0631, -0.0618, -0.0989, -0.0727],\n",
       "        [-0.0493,  0.1506,  0.0967, -0.0447, -0.1897, -0.0762,  0.0627, -0.0542, -0.0977, -0.0754],\n",
       "        [-0.0440,  0.1507,  0.1009, -0.0370, -0.1853, -0.0859,  0.0609, -0.0590, -0.0957, -0.0754],\n",
       "        [-0.0418,  0.1499,  0.0996, -0.0341, -0.1838, -0.0945,  0.0608, -0.0634, -0.0991, -0.0739],\n",
       "        [-0.0390,  0.1457,  0.1012, -0.0385, -0.1847, -0.0880,  0.0723, -0.0551, -0.0965, -0.0806],\n",
       "        [-0.0428,  0.1470,  0.1015, -0.0417, -0.1891, -0.0839,  0.0677, -0.0538, -0.0957, -0.0777]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the prediction with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
